{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443108ef-8549-4392-9078-9aaa5e547e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75d8b6e6-f62d-4c80-874a-801e607fbdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\siria\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\siria\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6525c9-a03e-49c9-8d64-3e834189f07d",
   "metadata": {},
   "source": [
    "### Stop words using nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0016520a-31df-42db-a3f7-c07b270002dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\siria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "976275dd-6854-49df-9107-a9bc730e828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'was', 'out', 'hasn', 'whom', 'just', 'all', 'be', 'who', 'himself', 'herself', 'your', 'they', 'until', 'this', 'for', 'on', 'should', \"hasn't\", 'me', 'why', 'i', 'after', 've', 'same', 'nor', 'did', 'has', 'again', 'them', 'll', 'down', 'have', 'because', 'yourselves', 'into', 'other', \"couldn't\", 's', 'some', 'most', 'or', 'yours', 'so', 'themselves', \"mightn't\", 'as', 'off', 'not', \"won't\", 'ain', 'too', 'my', 'over', 'it', 'which', \"it's\", 'and', 't', 'then', 'any', 'those', 'he', 'o', \"aren't\", 'wasn', 'isn', 'doing', \"you'll\", 'theirs', 'are', 'to', 'do', 'very', 'will', 'can', 'during', 'before', \"weren't\", 'hers', 'shan', 'his', 'about', \"don't\", 'how', \"doesn't\", 'haven', 'myself', 'is', 'needn', \"shouldn't\", 'does', 're', 'own', \"you've\", 'a', 'of', \"needn't\", 'won', \"should've\", \"she's\", 'doesn', \"shan't\", 'had', 'am', 'each', 'only', \"hadn't\", 'against', 'we', 'while', \"didn't\", 'being', 'where', 'm', 'from', 'itself', 'when', 'more', 'ours', 'her', 'no', \"that'll\", 'you', 'but', 'what', 'both', 'couldn', 'in', 'under', \"mustn't\", 'been', 'yourself', 'mustn', 'there', 'were', 'now', \"wasn't\", \"isn't\", 'don', 'ma', 'that', 'such', \"you're\", 'by', 'she', 'wouldn', \"you'd\", 'if', 'mightn', 'weren', 'below', 'with', 'their', 'these', 'at', 'having', \"wouldn't\", \"haven't\", 'aren', 'our', 'its', 'up', 'hadn', 'd', 'ourselves', 'than', 'him', 'the', 'above', 'few', 'y', 'didn', 'shouldn', 'through', 'here', 'once', 'an', 'further', 'between'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d500f-d577-4381-b42b-a3a56c7e0333",
   "metadata": {},
   "source": [
    "### Part -1 Implement and analyze word count.\n",
    "Implementation of basic word count 10 marks\n",
    "You must write a basic word count program that reads two text files, counts the number \n",
    "of occurrences of each word in the text files, and outputs the result back to a text file as \n",
    "a list of key-value pairs, where the key is the word, and the value is the number of times \n",
    "the word occurred in the text file named output_1.txt. Provide screen shot of your \n",
    "output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5522dc71-96b1-4519-8432-8affc8a5d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON']=sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=sys.executable\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "conf=SparkConf().setAppName(\"WordCountApp\").setMaster(\"local\")\n",
    "sc=SparkContext(conf=conf)\n",
    "\n",
    "#Split the line into words\n",
    "def split_words(line):\n",
    "    words=line.split()\n",
    "    return words\n",
    "file_path1=\"book1.txt\"\n",
    "file_path2=\"book2.txt\"\n",
    "\n",
    "text_rdd1=sc.textFile(file_path1)\n",
    "text_rdd2=sc.textFile(file_path2)\n",
    "\n",
    "#Combine the two RDDs\n",
    "text_rdd=text_rdd1.union(text_rdd2)\n",
    "\n",
    "#Flatten the list of words\n",
    "words_rdd=text_rdd.flatMap(split_words)\n",
    "\n",
    "#Map each word to a key-value pair\n",
    "word_counts_rdd=words_rdd.map(lambda word:(word, 1)).reduceByKey(lambda x,y:x + y)\n",
    "\n",
    "#Save the result to an output file\n",
    "output_path='output_1.txt'\n",
    "with open(output_path,'w',encoding='utf-8') as file:\n",
    "    for word,count in word_counts_rdd.collect():\n",
    "        file.write(f'{word}\\t{count}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b6b29-8027-4c09-b0b2-58456dcaccb1",
   "metadata": {},
   "source": [
    "### Extended code\n",
    "In addition to basic word counting, extend your code, which must also do the following: \n",
    "\n",
    "• It must be case insensitive (see lower() in Python)\n",
    "\n",
    "• It must ignore all punctuation (see, for example, translate() in Python)\n",
    "\n",
    "• It must ignore stop words (see filter() in Spark)\n",
    "\n",
    "• The output must be sorted by count in descending order (see sortBy() in Spark)\n",
    "To accomplish this, you will use a combination of basic Python and RDD operations in \n",
    "PySpark or Scala. The following programming guide goes over basics of getting started \n",
    "with Spark and should contain everything you need to complete this part of the \n",
    "homework: https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cab28c-f3a8-462e-8b5b-0c22ad10ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import string\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "os.environ['PYSPARK_PYTHON']=sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=sys.executable\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "    \n",
    "conf=SparkConf().setAppName(\"WordCountApp\").setMaster(\"local\")\n",
    "sc=SparkContext(conf=conf)\n",
    "\n",
    "# Defining a set of stop words to ignore from NLTK library\n",
    "stop_words=set(['was', 'out', 'hasn', 'whom', 'just', 'all', 'be', 'who', 'himself', 'herself', \n",
    "                  'your', 'they', 'until', 'this', 'for', 'on', 'should', \"hasnt\", 'me', 'why', \n",
    "                  'i', 'after', 've', 'same', 'nor', 'did', 'has', 'again', 'them', 'll', 'down', \n",
    "                  'have', 'because', 'yourselves', 'into', 'other', \"couldnt\", 's', 'some', 'most', \n",
    "                  'or', 'yours', 'so', 'themselves', \"mightnt\", 'as', 'off', 'not', \"wont\", 'ain', \n",
    "                  'too', 'my', 'over', 'it', 'which', \"its\", 'and', 't', 'then', 'any', 'those', 'he', \n",
    "                  'o', \"arent\", 'wasn', 'isn', 'doing', \"you'll\", 'theirs', 'are', 'to', 'do', 'very', \n",
    "                  'will', 'can', 'during', 'before', \"werent\", 'hers', 'shan', 'his', 'about', \"don't\", \n",
    "                  'how', \"doesnt\", 'haven', 'myself', 'is', 'needn', \"shouldnt\", 'does', 're', 'own', \n",
    "                  \"youve\", 'a', 'of', \"neednt\", 'won', \"shouldve\", \"shes\", 'doesn', \"shant\", 'had', \n",
    "                  'am', 'each', 'only', \"hadnt\", 'against', 'we', 'while', \"didnt\", 'being', 'where', \n",
    "                  'm', 'from', 'itself', 'when', 'more', 'ours', 'her', 'no', \"thatll\", 'you', 'but', \n",
    "                  'what', 'both', 'couldn', 'in', 'under', \"mustnt\", 'been', 'yourself', 'mustn', 'there', \n",
    "                  'were', 'now', \"wasnt\", \"isnt\", 'don', 'ma', 'that', 'such', \"youre\", 'by', 'she', 'wouldn', \n",
    "                  \"youd\", 'if', 'mightn', 'weren', 'below', 'with', 'their', 'these', 'at', 'having', \"wouldnt\", \n",
    "                  \"havent\", 'aren', 'our', 'its', 'up', 'hadn', 'd', 'ourselves', 'than', 'him', 'the', 'above', \n",
    "                  'few', 'y', 'didn', 'shouldn', 'through', 'here', 'once', 'an', 'further', 'between'])\n",
    "\n",
    "#Broadcasting stop words to make them available on all nodes\n",
    "broadcast_stop_words=sc.broadcast(stop_words)\n",
    "\n",
    "#Removing punctuations and converting all to lowercase\n",
    "def tokenize_and_clean(line):\n",
    "    line =line.translate(str.maketrans('','',string.punctuation))\n",
    "    return line.lower().split()\n",
    "\n",
    "#File paths for input text\n",
    "file_path1 = \"book1.txt\"\n",
    "file_path2 = \"book2.txt\"\n",
    "\n",
    "#Load both files into an RDD\n",
    "text_rdd1=sc.textFile(file_path1)\n",
    "text_rdd2=sc.textFile(file_path2)\n",
    "\n",
    "#Combine the two RDDs\n",
    "combined_text_rdd=text_rdd1.union(text_rdd2)\n",
    "\n",
    "#cleaning the text\n",
    "words_rdd=combined_text_rdd.flatMap(tokenize_and_clean)\n",
    "\n",
    "#Using filter() to remove stop words\n",
    "filtered_words_rdd=words_rdd.filter(lambda word:word not in broadcast_stop_words.value)\n",
    "\n",
    "#perform word count\n",
    "word_pairs_rdd=filtered_words_rdd.map(lambda word:(word, 1))\n",
    "word_counts_rdd=word_pairs_rdd.reduceByKey(lambda a,b:a+b)\n",
    "\n",
    "#Sorting by word count in descending order\n",
    "sorted_by_count_rdd=word_counts_rdd.sortBy(lambda x:x[1],ascending=False)\n",
    "\n",
    "top_25=sorted_by_count_rdd.take(25)\n",
    "\n",
    "#Collecting the data and saveing it to output_1.txt\n",
    "data = sorted_by_count_rdd.collect()\n",
    "with open('output_1_extended.txt', 'w', encoding='utf-8') as file:\n",
    "    for word, count in data:\n",
    "        file.write(f'{word}\\t{count}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2498eba-f638-4067-a0b0-fc0dfda84012",
   "metadata": {},
   "source": [
    "#### a. What are the 25 most common words? Include a screenshot of the program output to back up your claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fddc10f-d298-40a9-a2a7-cf8437301923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 Words:\n",
      "one: 452\n",
      "said: 424\n",
      "would: 420\n",
      "mrs: 397\n",
      "grace: 296\n",
      "like: 270\n",
      "mr: 262\n",
      "never: 252\n",
      "man: 241\n",
      "much: 239\n",
      "know: 217\n",
      "little: 214\n",
      "time: 212\n",
      "walter: 211\n",
      "well: 201\n",
      "say: 197\n",
      "could: 193\n",
      "see: 193\n",
      "young: 179\n",
      "miss: 179\n",
      "mordaunt: 177\n",
      "think: 174\n",
      "project: 166\n",
      "mother: 162\n",
      "dont: 160\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 results to verify\n",
    "print(\"Top 25 Words:\")\n",
    "for word, count in top_25:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15da5860-4e24-4fa9-8f2c-60c4bebe36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f98110df-4cc7-4dcc-944b-fa9c578d2bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340443d-d17f-49e4-b82e-4dc5c22e312a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981d484-7cfc-421d-8baf-15e3b4cb7464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b579d69-de42-4506-8bb8-871f45642b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

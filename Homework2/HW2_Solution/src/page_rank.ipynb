{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269073db-1580-4be7-9d7d-436ea65ca561",
   "metadata": {},
   "source": [
    "### Part-3 Implement and analyze Page-rank algorithm.\n",
    "1. You must write a basic page-rank algorithm considering the text file that is generated \n",
    "(question3.txt). It is a simulated network of 100 pages and its hyperlink .\n",
    "The algorithm should take the network provided and evaluate the page rank for all the \n",
    "webpages or nodes.\n",
    "2. Find the node with the highest and the lowest page rank and provide a screenshot of the \n",
    "same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6726eab0-ca60-4c0f-b043-972334fc9f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page with the lowest rank: (31, 0.002821903291492742)\n",
      "Page with the highest rank: (60, 0.02650135383327059)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "\n",
    "# Set environment variables for PySpark to use Python\n",
    "os.environ['PYSPARK_PYTHON']='python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='python'\n",
    "\n",
    "#saving the results to a text file\n",
    "def write_results(rdd_data,output_file_name):\n",
    "    output_data=rdd_data.collect()\n",
    "    with open(output_file_name,'w') as file:\n",
    "        for record in output_data:\n",
    "            file.write(f\"{record}\\n\")\n",
    "\n",
    "#calculating contributions from each node to its neighbors\n",
    "def compute_page_contributions(pair):\n",
    "    neighbors,rank_value=pair[1]\n",
    "    if len(neighbors)==0:\n",
    "        return []\n",
    "    contribution_value=rank_value/len(neighbors)\n",
    "    return [(neighbor,contribution_value) for neighbor in neighbors]\n",
    "\n",
    "#Adding contributions for each node\n",
    "def sum_contributions(contrib1,contrib2):\n",
    "    return contrib1+contrib2\n",
    "\n",
    "#applying the damping factor(beta) to the summed contributions\n",
    "def apply_damping_factor(total_contrib):\n",
    "    damping=0.85\n",
    "    return (total_contrib*damping)+(1-damping)/total_pages\n",
    "\n",
    "#Formatting the final output before saving to a file\n",
    "def format_rank_result(record):\n",
    "    return f\"{record[0]}, {record[1]}\"\n",
    "\n",
    "\n",
    "spark_conf=SparkConf().setAppName(\"PageRankCalculation\").setMaster(\"local\").set(\"spark.pyspark.python\", \"python\").set(\"spark.pyspark.driver.python\", \"python\")\n",
    "sc=SparkContext(conf=spark_conf)\n",
    "\n",
    "#lines_rdd=sc.textFile(\"question3.txt\").map(lambda line:line.strip())\n",
    "lines_rdd=sc.textFile(\"question3.txt\")\n",
    "\n",
    "#Parse each line to extract page\n",
    "edges_rdd=lines_rdd.map(lambda line: (\n",
    "    int(line.split(':')[0].strip()),\n",
    "    [int(neighbor.strip(' []')) for neighbor in line.split(':')[1].split(',')]\n",
    "))\n",
    "\n",
    "#Counting the total number of unique pages (nodes)\n",
    "total_pages=edges_rdd.count()\n",
    "\n",
    "#Initializing the rank of each page to 1/total_pages\n",
    "page_rank_initial=edges_rdd.map(lambda page: (page[0], 1.0/total_pages))\n",
    "\n",
    "#Defining the maximum number of iterations for PageRank\n",
    "max_iterations = 8\n",
    "iteration_count = 0\n",
    "\n",
    "while iteration_count < max_iterations:\n",
    "    #Joining edges data with current page ranks and compute contributions\n",
    "    contrib_rdd=edges_rdd.join(page_rank_initial).flatMap(compute_page_contributions)\n",
    "    #Updating the page ranks using the summed contributions\n",
    "    page_rank_initial=contrib_rdd.reduceByKey(sum_contributions).mapValues(apply_damping_factor)\n",
    "    iteration_count += 1\n",
    "\n",
    "#Find the node with the lowest rank\n",
    "min_rank_node=page_rank_initial.min(lambda x:x[1])\n",
    "\n",
    "#Find the node with the highest rank\n",
    "max_rank_node=page_rank_initial.max(lambda x:x[1])\n",
    "\n",
    "print(\"Page with the lowest rank:\",min_rank_node)\n",
    "print(\"Page with the highest rank:\",max_rank_node)\n",
    "\n",
    "formatted_rank_output=page_rank_initial.map(format_rank_result)\n",
    "write_results(formatted_rank_output, 'output_page_rank.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dfae71-5406-4ea4-a024-2d686c680dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
